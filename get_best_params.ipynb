{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import nltk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df_tv = pd.read_csv('data/train_folds.csv')\n",
    "df_tv['Phrase'] = df_tv['Phrase'].apply(lambda s: s.replace(\"n't\", 'not'))\n",
    "\n",
    "classifiers = {\n",
    "    # 'lr': LogisticRegression,\n",
    "    # 'svc': SVC, # too slow!!!\n",
    "    # 'lsvc': LinearSVC,\n",
    "    # 'mnb': MultinomialNB,\n",
    "    # 'knn': KNeighborsClassifier, # too slow!!!\n",
    "    # 'dt': DecisionTreeClassifier, # doesn't work\n",
    "    # 'rf': RandomForestClassifier, # doesn't work\n",
    "    # 'ada': AdaBoostClassifier, # too slow!!!\n",
    "    # 'gb': GradientBoostingClassifier, # too slow!!!\n",
    "    # 'sgd': SGDClassifier\n",
    "}\n",
    "\n",
    "classifiers_params = {\n",
    "    'lr': {\n",
    "        'C': np.logspace(-3, 3, 7)\n",
    "    },\n",
    "    'svc': {\n",
    "        # 'C': np.logspace(-3, 3, 7),\n",
    "        # 'gamma': np.logspace(-3, 1, 5)\n",
    "    },\n",
    "    'lsvc': {\n",
    "        'C': np.logspace(-3, 3, 7)\n",
    "    },\n",
    "    'mnb': {\n",
    "        'alpha': np.linspace(0, 1, 11)\n",
    "    },\n",
    "    'knn': {\n",
    "        'n_neighbors': np.linspace(3, 7, 5, dtype=int)\n",
    "    },\n",
    "    'dt': {\n",
    "        'criterion': [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        'min_samples_leaf': [0.1],\n",
    "        'max_depth': np.linspace(3, 7, 5, dtype=int)\n",
    "    },\n",
    "    'rf': {\n",
    "        ''\n",
    "        'criterion': [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        'min_samples_leaf': [0.1],\n",
    "        'max_depth': np.linspace(3, 7, 5, dtype=int),\n",
    "        'n_estimators': np.linspace(50, 300, 6, dtype=int)\n",
    "    },\n",
    "    'ada': {\n",
    "        'n_estimators': np.linspace(50, 300, 6, dtype=int)\n",
    "    },\n",
    "    'gb': {\n",
    "        'n_estimators': np.linspace(50, 300, 6, dtype=int),\n",
    "        'min_samples_leaf': [0.1],\n",
    "    },\n",
    "    'sgd': {\n",
    "        'penalty' : ['l2', 'l1', 'elasticnet'],\n",
    "        'alpha': np.logspace(-7, -1, 7)\n",
    "    }\n",
    "}\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text: str):\n",
    "    text = text.lower()\n",
    "    tokens = [word for word in nltk.word_tokenize(text) if word.isalpha() and word not in ENGLISH_STOP_WORDS]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "def get_accuracy(classifier, params, kfold):\n",
    "    df_tr = df_tv[df_tv['kfold'] != kfold]\n",
    "    df_vl = df_tv[df_tv['kfold'] == kfold]\n",
    "\n",
    "    ytr = df_tr['Sentiment']\n",
    "    yvl = df_vl['Sentiment']\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(\n",
    "        token_pattern=r'\\b[^\\d\\W][^\\d\\W]+\\b',\n",
    "        ngram_range=(1, 2),\n",
    "        max_df=0.9,\n",
    "        lowercase=True\n",
    "    ).fit(df_tr['Phrase'])\n",
    "\n",
    "    xtr = vectorizer.transform(df_tr['Phrase'])\n",
    "    xvl = vectorizer.transform(df_vl['Phrase'])\n",
    "\n",
    "    model = classifier(**params).fit(xtr, ytr)\n",
    "    ypd = model.predict(xvl)\n",
    "    accuracy = metrics.accuracy_score(yvl, ypd)\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, classifier in classifiers.items():\n",
    "    print(f'{name} classifier:')\n",
    "    print('*'*100)\n",
    "    for params in model_selection.ParameterGrid(classifiers_params[name]):\n",
    "        accuracy = mean([get_accuracy(classifier, params, kfold) for kfold in range(5)])\n",
    "        print(f'average accuracy: {accuracy} | params: {params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d597cde149abeb15f6ee103d6835e72487950cb0d72203742dce2097b2edee99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
